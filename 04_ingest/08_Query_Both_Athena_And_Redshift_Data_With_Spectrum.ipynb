{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Both Athena And Redshift With `Redshift Spectrum`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage our previously created table in Amazon Athena with its metadata and schema information stored in the AWS Glue Data Catalog to access our data in S3 through Redshift Spectrum. All we need to do is create an external schema in Redshift, point it to our AWS Glue Data Catalog, and point Redshift to the database we’ve created.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/redshift_spectrum.png\" width=\"90%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Get region \n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Redshift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = boto3.client('redshift')\n",
    "secretsmanager = boto3.client('secretsmanager')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Redshift Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "secret = secretsmanager.get_secret_value(SecretId='dsoaws_redshift_login')\n",
    "cred = json.loads(secret['SecretString'])\n",
    "\n",
    "master_user_name = cred[0]['username']\n",
    "master_user_pw = cred[1]['password']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redshift Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_cluster_identifier = 'dsoaws'\n",
    "\n",
    "database_name_redshift = 'dsoaws'\n",
    "database_name_athena = 'dsoaws'\n",
    "\n",
    "redshift_port = '5439'\n",
    "\n",
    "schema_redshift = 'redshift'\n",
    "schema_athena = 'athena'\n",
    "\n",
    "table_name_tsv = 'amazon_reviews_tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait for Cluster Status  `Available`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "print(cluster_status)\n",
    "\n",
    "while cluster_status != 'available':\n",
    "    time.sleep(10)\n",
    "    response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "    cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "    print(cluster_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Redshift Endpoint Address & IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_endpoint_address = response['Clusters'][0]['Endpoint']['Address']\n",
    "iam_role = response['Clusters'][0]['IamRoles'][0]['IamRoleArn']\n",
    "\n",
    "print('Redshift endpoint: {}'.format(redshift_endpoint_address))\n",
    "print('IAM Role: {}'.format(iam_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Redshift Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "\n",
    "con_redshift = wr.data_api.redshift.connect(\n",
    "    cluster_id=redshift_cluster_identifier,\n",
    "    database=database_name_redshift,\n",
    "    db_user=master_user_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redshift Spectrum\n",
    "Amazon Redshift Spectrum directly queries data in S3, using the same SQL syntax of Amazon Redshift. You can also run queries that span both the frequently accessed data stored locally in Amazon Redshift and your full datasets stored cost-effectively in S3.\n",
    "\n",
    "To use Redshift Spectrum, your cluster needs authorization to access data catalog in Amazon Athena and your data files in Amazon S3. You provide that authorization by referencing an AWS Identity and Access Management (IAM) role that is attached to your cluster. \n",
    "\n",
    "To use this capability in from your Amazon SageMaker notebook:\n",
    "\n",
    "* Register your Athena database `dsoaws` with Redshift Spectrum\n",
    "* Query Your Data in Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Redshift\n",
    "\n",
    "Let's query results across Athena and Redshift tables using just Redshift.  This feature is called Redshift Spectrum.  We will use a `UNION ALL` for this.  Similarly, if we need to delete data, we would drop the tables using `UNION ALL`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `UNION ALL` across 2 tables (2015, 2014) in our `redshift` schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM redshift.amazon_reviews_tsv_2015\n",
    "  GROUP BY redshift.amazon_reviews_tsv_2015.product_category, year\n",
    "UNION ALL\n",
    "SELECT year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM redshift.amazon_reviews_tsv_2014\n",
    "  GROUP BY redshift.amazon_reviews_tsv_2014.product_category, year\n",
    "ORDER BY product_category ASC, year DESC\n",
    "\"\"\"\n",
    "\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wr.data_api.redshift.read_sql_query(\n",
    "    sql=statement,\n",
    "    con=con_redshift,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Same Query on Original Data in S3 using `athena` Schema to Verify  the Results Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT CAST(DATE_PART_YEAR(TO_DATE(review_date, 'YYYY-MM-DD')) AS INTEGER) AS year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM athena.amazon_reviews_tsv\n",
    "  WHERE year = 2015 OR year = 2014 \n",
    "  GROUP BY athena.amazon_reviews_tsv.product_category, year\n",
    "ORDER BY product_category ASC, year DESC\n",
    "\"\"\"\n",
    "\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wr.data_api.redshift.read_sql_query(\n",
    "    sql=statement,\n",
    "    con=con_redshift,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Query Across Both Redshift and Athena in a single query\n",
    "\n",
    "Use `UNION ALL` across 2 Redshift tables (2015, 2014) and the rest from Athena/S3 (2013-1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM redshift.amazon_reviews_tsv_2015\n",
    "  GROUP BY redshift.amazon_reviews_tsv_2015.product_category, year\n",
    "UNION ALL\n",
    "SELECT year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM redshift.amazon_reviews_tsv_2014\n",
    "  GROUP BY redshift.amazon_reviews_tsv_2014.product_category, year\n",
    "UNION ALL\n",
    "SELECT CAST(DATE_PART_YEAR(TO_DATE(review_date, 'YYYY-MM-DD')) AS INTEGER) AS year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM athena.amazon_reviews_tsv\n",
    "  WHERE year <= 2013\n",
    "  GROUP BY athena.amazon_reviews_tsv.product_category, year\n",
    "ORDER BY product_category ASC, year DESC\n",
    "\"\"\"\n",
    "\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wr.data_api.redshift.read_sql_query(\n",
    "    sql=statement,\n",
    "    con=con_redshift,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `EXPLAIN` to Verify that Both Redshift and S3 are Part of the Same Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "EXPLAIN \n",
    "SELECT year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM redshift.amazon_reviews_tsv_2015\n",
    "  GROUP BY redshift.amazon_reviews_tsv_2015.product_category, year\n",
    "UNION ALL\n",
    "SELECT year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM redshift.amazon_reviews_tsv_2014\n",
    "  GROUP BY redshift.amazon_reviews_tsv_2014.product_category, year\n",
    "UNION ALL\n",
    "SELECT CAST(DATE_PART_YEAR(TO_DATE(review_date, 'YYYY-MM-DD')) AS INTEGER) AS year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM athena.amazon_reviews_tsv\n",
    "  WHERE year <= 2013\n",
    "  GROUP BY athena.amazon_reviews_tsv.product_category, year\n",
    "ORDER BY product_category ASC, year DESC\n",
    "\"\"\"\n",
    "\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 1024)\n",
    "\n",
    "df = wr.data_api.redshift.read_sql_query(\n",
    "    sql=statement,\n",
    "    con=con_redshift,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "```\n",
    "QUERYPLAN\n",
    "XN Merge  (cost=1000177373551.14..1000177373584.69 rows=13420 width=1040)\n",
    "  Merge Key: product_category, year\n",
    "  ->  XN Network  (cost=1000177373551.14..1000177373584.69 rows=13420 width=1040)\n",
    "        Send to leader\n",
    "        ->  XN Sort  (cost=1000177373551.14..1000177373584.69 rows=13420 width=1040)\n",
    "              Sort Key: product_category, year\n",
    "              ->  XN Append  (cost=733371.52..177372631.06 rows=13420 width=1040)\n",
    "                    ->  XN Subquery Scan *SELECT* 1  (cost=733371.52..733372.06 rows=43 width=22)\n",
    "                          ->  XN HashAggregate  (cost=733371.52..733371.63 rows=43 width=22)\n",
    "                                ->  XN Seq Scan on amazon_reviews_tsv_2015  (cost=0.00..419069.44 rows=41906944 width=22)\n",
    "                    ->  XN Subquery Scan *SELECT* 2  (cost=772258.45..772258.98 rows=43 width=23)\n",
    "                          ->  XN HashAggregate  (cost=772258.45..772258.55 rows=43 width=23)\n",
    "                                ->  XN Seq Scan on amazon_reviews_tsv_2014  (cost=0.00..441290.54 rows=44129054 width=23)\n",
    "                    ->  XN Subquery Scan *SELECT* 3  (cost=175866766.67..175867000.02 rows=13334 width=1040)\n",
    "                          ->  XN HashAggregate  (cost=175866766.67..175866866.68 rows=13334 width=1040)\n",
    "                                ->  XN S3 Query Scan amazon_reviews_tsv  (cost=175000000.00..175766766.67 rows=13333334 width=1040)\n",
    "                                      Filter: (date_part_year(to_date((derived_col1)::text, 'YYYY-MM-DD'::text)) <= 2013)\n",
    "                                      ->  S3 HashAggregate  (cost=175000000.00..175000100.00 rows=40000000 width=1036)\n",
    "                                            ->  S3 Seq Scan athena.amazon_reviews_tsv location:s3://sagemaker-us-west-2-237178646982/amazon-reviews-pds/tsv format:TEXT  (cost=0.00..100000000.00 rows=10000000000 width=1036)\n",
    "----- Tables missing statistics: amazon_reviews_tsv_2015, amazon_reviews_tsv_2014 -----\n",
    "----- Update statistics by running the ANALYZE command on these tables -----\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use Athena vs. Redshift?\n",
    "\n",
    "### Amazon Athena\n",
    "Athena should be your preferred choice when running ad-hoc SQL queries on data that is stored in Amazon S3. It doesn’t require you to set up or manage any infrastructure resources, and you don’t need to move any data. It supports structured, unstructured, and semi-structured data. With Athena, you are defining a **“schema on read”** - you basically just log in, create a table and you are good to go. \n",
    "\n",
    "### Amazon Redshift\n",
    "Redshift is targeted for modern data analytics on large sets of structured data. Here, you need to have a predefined **“schema on write”**. Unlike serverless Athena, Redshift requires you to create a cluster (compute and storage resources), ingest the data and build tables before you can start to query, but caters to performance and scale. So for any highly-relational data with a transactional nature (data gets updated), workloads which involve complex joins, and latency requirements to be sub-second, Redshift is the right choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
